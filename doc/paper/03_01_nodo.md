## Nodo de roteamento com *Actor Model*
\label{n2:nodo_actor}

O nodo deve ser desenvolvido sobre o protocolo P2P específico para o *Bitcoin*, conforme informado no~\REF{n3:proto_node}. Este protocolo é divido em duas partes: topologia do nodo em *Actor Model* e Protocolo do nodo, no qual a primeira refere-se apenas à topologia escolhida  para tratar com assincronismo, e a segunda é referente às funcionalidades esperadas por um nodo da rede do *Bitcoin*.

### Topologia do nodo em *Actor Model*
\label{n3:nodo_topologia_actor}

O programa quando em execução é um potencial nodo da rede *Peer-to-Peer* do *Bitcoin*. Como um *actor*, mantém três interfaces: escrita do *log* em um arquivo; escuta por novas conexões TCP por aplicações *telnet* para administradores do nodo em uma porta; e escuta por novas conexões TCP por aplicações de outros nodos em outra porta. O seu comportamento, ao receber novas conexões administrativas ou de nodos, é o de criar novos *actors* de *admin* e de *peer*, respectivamente. Para evitar ambiguidade, é utilizado "nodo" para designar um programa executando em algum computador e "*peer*" para designar a representação interna (*actor*) de um nodo externo.

#### *Actors* em uma rede P2P
\label{n3:actors_p2p}

\begin{figure}[!ht]
	\centering
	\caption{Usuários e *actors* em nodos diferentes.}
	\includegraphics[width=0.7\textwidth]{peers.png}
	\begin{minipage}{\textwidth}
		\centering
		{\small Fonte: Autoria própria.}\par
	\end{minipage}
	\label{fig:nodos}
\end{figure}

Na~\REF{fig:nodos}--é mostrado uma topologia de conexão entre três nodos, e cada administrador, de uma cor, controla um nodo da mesma cor. Cada nodo mantém um *actor* do seu administrador, uma representação interna ao nodo. O nodo azul mantém uma conexão com os outros nodos, e representa cada um dos outros nodos através de *actors*, e os outros nodos criam tais representações de forma semelhante. 

As informações salvas no arquivo de *log* são interferidas por *macros* na qual podem, através do compilador padrão, informar o horário, módulo, nível de depuração[^20], arquivo e linha em que o comando se encontra no código.[^21] Para isto as *macros* expandem em código que contém tais informações, efeito que contribui para a produtividade do desenvolvimento do programa.

[^20]: *Logs* podem ser completamente omitidos de forma seletiva de acordo com o seu nível de depuração.

[^21]: *E.g.* *[17:40:32][btc][INFO] [/main.rs:101] New admin connection: V4(127.0.0.1:60916)*.

#### Topologia Simplificada
\label{n4:topologia_simpl}

Para a análise da topologia de interação entre os *actors* internos ao sistema, convém uma representação gráfica delineando todos os tipos de *actors* e seus canais de comunicação.

\begin{figure}[!ht]
	\centering
	\caption{Nodo simplificado utilizando *Actor Model*.}
	\includegraphics[width=0.7\textwidth]{server_simples.png}
	\begin{minipage}{\textwidth}
		\centering
		{\small Fonte: Autoria própria.}\par
	\end{minipage}
	\label{fig:nodo_simples}
\end{figure}

A~\REF{fig:nodo_simples}--representa as principais interações entre os *actors* do sistema. Pela natureza isolada de *actors*, o compartilhamento de informação acontece principalmente por comunicação através de canais de mensagens.

Os *admins*/*peers* não tem nenhuma interferência que não aquelas de seus canais de comunicação e de *sockets* que veem de usuários/nodos externos.
O *scheduler* e os *workers* também interagem com outros *actors* através de canais de comunicação. No entanto todos os *workers* tem acesso interno ao *toolbox*, que é armazenado no *scheduler* e é utilizado de maneira similar a uma variável global que fornece acessos e informações pertinentes sobre o nodo. O *toolbox* é composto por partes menores que, cada uma, tem seu próprio acesso mutualmente exclusivo, delimitado por um *mutex*, prevenindo *data-races* em um nível de acesso controlado.
Com esta topologia de comunicação, distribuição de execução e acesso a dados, muitas execuções concorrentes serão possíveis entre *workers* diferentes, desde que não precisem de um acesso exclusivo a uma mesma variável.[^22]

[^22]: *RwLock* é uma delimitação alternativa ao *mutex* que permite múltiplos acessos concorrente exclusivamente de leitura.

O *scheduler*, em particular, é criado na inicialização do programa e termina apenas na terminação geral do programa, e uma de suas funções é coordenar os pedidos de execuções--assíncronos e concorrentes--que serão enviados pelos demais *actors*, que podem ter especificações de prioridade e sempre esperam por alguma resposta. Outra importante tarefa do *scheduler* é executar a política de criação, remoção e utilização de *workers*, *actors* que apenas executam o que lhes é pedido e produzem uma resposta de forma assíncrona.

\begin{figure}[!ht]
	\centering
	\caption{Nodo utilizando *Actor Model*.}
	\includegraphics[width=0.7\textwidth]{server_completo.png}
	\begin{minipage}{\textwidth}
		\centering
		{\small Fonte: Autoria própria.}\par
	\end{minipage}
	\label{fig:nodo_completo}
\end{figure}

#### Topologia Completa
\label{n4:topologia_compl}

A~\REF{fig:nodo_completo}--representa as principais interações entre os *actors* do sistema. Pela natureza de isolamento dos *actors*, o compartilhamento de informação acontece principalmente por comunicação através de canais unidirecionais de mensagens. 

##### *Admins* e *Peers*
\label{n5:admin_peers}

Logo após a inicialização, não há nenhum *worker*, nenhum *admin* e nenhum *peer*, há apenas o *scheduler* e o *actor* do nodo. O *actor* do nodo recebe as novas conexões TCP em portas específicas, e inicia a configuração de um novo *actor*. Seja um administrador ou um outro nodo, este *admin/peer actor*, de início, tem três canais que existirão ao longo da existência do próprio *admin/peer actor*: *chn msg r* (azul), *sched w* (preto) e *chn reg w* (verde). O primeiro canal possibilita que o *admin/peer actor* receba mensagens enviadas por um *worker*, *e.g.* como um efeito de um pedido realizado por um *admin*. O segundo canal permite que o *peer* envie pedidos de execução que serão realizados pelos *workers*. O terceiro canal tem uma função administrativa perante o *scheduler*, utilizado somente em momentos de cadastramento ou descadastramento de *actors*[^24].

[^24]: É com o uso deste canal que um *admin* pode iniciar uma nova conexão com outro nodo, ao realizar um pedido a um *worker* de início de uma conexão TCP que, em sucesso, cria um novo *peer*. Como o *worker* deverá cadastrar este *peer* no *scheduler*, o *admin* envia junto ao próprio pedido uma cópia da capacidade de transmissão do canal *mpsc* (*multiple producer, single consumer*) de des/cadastro. O *worker*, nesta situação, não guarda esta capacidade de des/cadastro consigo, mas a transfere para o novo *peer* que está sendo criado.
    Também é com o uso deste canal que o *admin/peer* podem se descadastrar junto ao *scheduler*.
    Dado esta dinâmica possibilitada pelos pedidos, a topologia do nodo é controladamente variável, pois existem possibilidades de interações temporárias/momentâneas.

Cada pedido realizado por um *admin/peer* implica em uma resposta, mesmo que ela não seja significativa para o *admin/peer*. Ao elaborar o pedido, o *actor* cria um canal *oneshot*[^25], armazena a capacidade de receber o valor que será transmitido pelo canal (nomeado *shot r* e em lilás) e envia a capacidade de transmitir o valor junto ao próprio pedido (nomeado *shot w*, em lilás, e armazenado no *scheduler*, bloco *A*).

[^25]: De uso único.

##### *Scheduler*
\label{n5:sched}

O *scheduler* é o *actor* de interface entre os *admin/peer* e os *workers*. Ele recebe vários pedidos de vários *admins/peers* através do *sched r* (em preto), lida com a carga de trabalho dos *workers* e também com a sua criação e destruição (mostrado como o componente *select*, em preto), encaminha os pedidos aos *workers* através dos *exec w* (em preto), recebe a resposta dos *workers* através dos *shot r* (em lilás, no bloco *B*) e encaminha as respostas de volta para os *admins/peers* através do *shot w* (em lilás, no bloco *A*). De maneira similar ao *admin/peer*, o *scheduler* cria um canal *oneshot* para receber uma resposta de um *worker*, e encaminha esta nova capacidade de escrita para o *worker*[^32].

[^32]: Reaproveitando a memória alocada do pedido realizada pelo *admin/node*.

No *scheduler*, o bloco *A* representa uma listagem de vários canais referentes a cada *admin/peer*: um *sched r* para cada *peer/admin* e um *shot w* para cada resposta esperada por cada *admin/peer*[^27]. O bloco *B* representa uma listagem de vários canais referentes a cada *worker*: um *exec w* para cada *worker* e um *shot r* para cada pedido em execução, ou em fila de execução, por cada *worker*.[^28]

[^27]: Um *admin/peer* pode realizar vários pedidos e esperar por todos eles ao mesmo tempo, mantendo vários *shot r* consigo.

[^28]: Um *worker* pode internamente conter uma fila de pedidos, mas executa um por vez. Este arranjo permite que exista uma reserva de *workers* com uma reserva de *threads* com uma reserva de *cores* de processamento para execuções prioritárias, caso seja de interesse do gerenciador do nodo--porém, neste caso, a implementação do *scheduler* precisaria ser generalizada em alguns pontos.

*Scheduler* armazena uma alternativa às variáveis globais: o *toolbox*, variável que é compartilhada[^29] entre *workers*, fornece um acesso indireto a estruturas de acesso exclusivo e referentes ao nodo como um todo, como explanado em \REF{n4:topologia_simpl}. O *toolbox* conterá várias variáveis que poderão ser acessadas pelos *workers*, mas por enquanto apenas a *PeerMsgr*, ou *Peer Messenger*, foi criada, pois foi considerada indispensável para a topologia considerada. O *Peer Messenger* tem uma parte transmissora de vários canais, um para cada *admin/peer*.[^30]

[^29]: Através de um *Arc* (*Atomic Reference Counter*), um acesso indireto que pode ser compartilhado entre *threads* diferentes. Esta caracterização da variável importa pois a desalocação de recursos no *Rust* é determinística, e o recurso referente ao *toolbox* é desalocado quando nenhum *worker* e nem o *scheduler* estiverem referenciando o *toolbox*. Como a contagem automática de referência (*Rc*, *Reference Counter*) precisa ser sincronizada entre *threads* diferentes, tal operação, neste caso, requer o custo adicional de ser atômica.

[^30]: *E.g.* quando um *admin* requisita a desconexão e remoção estável de um *peer*. Nesta caso, um *worker* recebe e executa este pedido, e retorna uma resposta ao *admin*. Antes de responder, o *worker* acessa o *toolbox* e adquire exclusividade temporária de acesso sobre o *Peer Messenger*--esperando de forma síncrona caso tal estrutura esteja em uso (*locked*). Ao ter acesso exclusivo sobre o *Peer Messenger*, o *worker* envia uma mensagem de comando de alta prioridade para o *peer* a ser removido--de que o *peer* deve se preparar para se remover--, e então desaloca a parte transmissora deste próprio canal com o *peer*, e também descadastra este *peer* do *Peer Messenger*. O *peer*, ao entrar em um estado neutro (ver o estado *standby* em \REF{n4:mach_peer_actor}) inicia alguns procedimentos internos a ele--como continuar esperando respostas, mesmo que insignificantes, já existentes a retornarem a ele--para que ele possa ser removido com segurança: ele próprio pedir seu descadastramento junto ao *scheduler* e então entrar em um estado terminal, com a desalocação e desligamento de todos os demais recursos, como canais de *socket*.

Por fim, o *scheduler* não interpreta nenhum pedido ou resposta. Isto simplifica a elaboração, execução concorrente e reutilização de pedidos por diferentes *actors*. Pelo fato do *scheduler* não precisar fazer esta interpretação, o *workflow* dos pedidos e respostas é simplificado, e o comportamento dos *actors* se torna mais previsível.

##### *Worker*
\label{n5:worker}

O *worker* é um *actor* que interage com o *scheduler* e *admins/peers* (estes últimos indiretamente). Mantém uma lista de pedidos a serem executados e executa o que for de maior prioridade. Após cada execução, o *worker* adquire pedidos novos enviados a ele pelo *exec r* (em preto), os ordenando na lista de pedidos. Depois da execução, prepara a resposta e a envia através do *shot w* (em lilás). Cada pedido é tratado isoladamente--o *worker* não tem alteração interna de estado como efeito de um pedido--, sendo desestruturado e então interpretado. Para a posterioridade, é esperado que novos tipos de *actors* existirão, e que poderão interagir com os canais do *toolbox*. Neste caso, os *workers* poderão precisar de uma implementação por máquina de estado para que possam interagir com novos *actors* de forma assíncrona.

### Protocolo do Nodo
\label{n3:proto_nodo}

O protocolo na implementação do nodo é dividido em três partes: *codec*, *peer actor*--representação interna de nodos externos--e *admin actor* que é a representação do usuário administrador conectado ao nodo, e por meio deste serão executados os comandos referentes à carteira.

#### *Codec*
\label{n4:nodo_codec}

O *codec* é um componente que gerencia o *socket* e realiza a abstração de dados, sendo assim, é preciso existir um tipo de *codec* para cada tipo de *actor* diferente que interage com um *socket*, um para o *admin* e outro para o *peer*. O *codec* do *admin* não possui funções de abstração de dados, pois apenas lida com a interface *telnet*, sendo assim apenas lê o *socket* e gera um *string* até ler uma quebra de linha e quando necessário envia no *socket* uma *string*, utilizado para *feedback*.

O *codec* do *peer* trabalha com um *socket* direto com outro nodo onde as mensagem estão em bytes, e ele realiza o processo de abstração desta mensagem em estruturas que possuem significado (mais alto nível). A codificação ocorre apenas na construção de mensagem feita pelo *actor* do *worker* e a decodificação é feita ao receber uma mensagem no *socket*, ocorre quando o nodo referente àquele *peer* manda uma mensagem para o nodo.

O processo de decodificação começa quando chega uma quantidade de bytes no *socket* do *peer*, então é verificado se existem pelo menos 24 bytes de informação (tamanho do *header*) e caso existam é verificado se os bytes referentes à cada uma das variáveis do *header* esta dentro do esperado[^2] e com estas variáveis é construído o *header*. Com o valor da variável *payload size* será lido esta quantidade exata de bytes do *socket*, caso existam, e se os *checksums*[^1] forem iguais é criado a abstração do *payload* e assim acaba o processo, pois já existe as abstrações necessárias para se construir a mensagem abstraída.

[^2]: As variáveis *command* e *magic bytes* possuem um conjunto definido de valores.

[^1]:*Payload* *checksum* informado no *header* e *checksum* do *payload* lido.

O processo de codificação é iniciado quando chega no canal do *worker* uma tarefa de envio de mensagem para algum nodo, é criado a abstração do *payload*, feita de acordo com o comando especificado, e gerado os bytes deste *payload* assim é calculado o seu *checksum* e seu tamanho e gerado a abstração do *header* a partir destas informações, agora é necessário apenas gerar o hexadecimal do *header*, a mensagem serializada é a concatenação do hexadecimal do *header* e do *payload*.

#### *Peer Actor*
\label{n4:mach_peer_actor}

Sempre quando é iniciado uma nova conexão com algum nodo é criado um *peer*, e após isto é iniciado a máquina de estados deste *actor*. 
Nesta máquina o seu estado inicial, chamado de *standby*, fica à espera de alguma notificação do *codec*. 
Assim que é recebido esta notificação são tratados os três canais que ele possui: o do *socket*, o do *scheduler*, canal que possui o *feedback* do *worker* em relação ao pedido realizado e pode ter a sua mensagem ignorada[^3], e o do *toolbox*, canal referente às tarefas que o *peer* deve realizar, enviadas diretamente pelo *worker*[^4].

No estado de *standby*, após ser recebido qualquer notificação, são descartados todas as respostadas provenientes do *scheduler* e é verificado o conteúdo do *toolbox* e são executadas todas as tarefas existentes nele. A maioria das tarefas serão com relação ao envio de mensagem para o nodo relativo ao *peer*, no entanto mensagens diferentes precisam ser tratadas de forma diferentes, de acordo com o protocolo do *Bitcoin*, por exemplo, caso seja recebido uma tarefa de *ping* é necessário que seja esperado uma mensagem de resposta de *pong* do nodo.

Como existem diversos tipos de comandos e de protocolos, que podem ser demasiadamente complexos, é trocado de estado e este novo estado irá definir como deverá ser a resposta e o que deverá ser esperado. Para os procedimentos mais complexos é iniciado um estado que irá iniciar uma nova máquina de estados interna para tratá-los, facilitando assim o entendimento. Todos este procedimento é cíclico, terminando apenas quando esta conexão *peer-nodo* acaba.

[^3]: A mensagem pode ser ignorada pois este retorno não é a resposta de nenhuma ação e sim das comunicações, portanto não há necessidade de ser enviada para o outro nodo ou informado no servidor.

[^4]: Um exemplo de tarefa é a *rawmsg*, onde deve ser enviado a mensagem hexadecimal recebida neste canal para o *socket*.

#### *Admin Actor*
\label{n4:mach_admin_actor}

De maneira similar ao *actor* do *peer*, existe uma máquina de estados para controlá-lo com a diferença de que a sua conexão não é com um nodo de Bitcoin mas sim com uma interface *telnet*. Assim, este também permanece no estado inicial esperando alguma notificação do *codec*. Este *actor*, no entanto, possui apenas dois canais de comunicação: o do *scheduler*, sendo que o *feedback* do *scheduler* neste caso é importante[^5], e o do *socket*.

[^5]: Informar os resultados do comando executado pelo *worker* na interface *telnet*.

Quando chega alguma informação no *socket* do *admin* gerenciado pelo *codec*, a máquina é notificada e começa o procedimento. O primeiro passo é comparar a *string* obtida com o argumento pré-definido construído em *struct-opt* e, caso sejam iguais, é criado a estrutura da requisição e é modificado de estado. Caso contrário, é apresentado o *help* do comando que falhou.

Na maioria dos casos o pedido do *admin* possui um *workflow* simples, onde é enviado uma requisição ao *scheduler* e ele espera o *feedback* para enviar ao *codec*. Após isto, ele retorna ao estado inicial e, para estes casos, é definido um estado chamado de *simpleWait* que define este comportamento. Para o restante dos casos, assim como na máquina do *peer*, é criado um estado para cada tipo de pedido e, caso exista um muito complexo, é utilizada uma máquina de estados interna.
